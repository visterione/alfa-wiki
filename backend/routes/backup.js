const express = require('express');
const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs').promises;
const fsSync = require('fs');
const archiver = require('archiver');
const unzipper = require('unzipper');
const multer = require('multer');
const { authenticate, requireAdmin } = require('../middleware/auth');
const { sequelize } = require('../models');

const router = express.Router();

const BACKUP_DIR = process.env.BACKUP_PATH || path.join(__dirname, '..', 'backups');
const UPLOADS_DIR = path.join(__dirname, '..', 'uploads');

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MULTER SETUP
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const storage = multer.diskStorage({
  destination: async (req, file, cb) => {
    await fs.mkdir(BACKUP_DIR, { recursive: true });
    cb(null, BACKUP_DIR);
  },
  filename: (req, file, cb) => {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    cb(null, `uploaded-${timestamp}.zip`);
  }
});

const upload = multer({
  storage,
  limits: { fileSize: 500 * 1024 * 1024 },
  fileFilter: (req, file, cb) => {
    if (file.mimetype === 'application/zip' || 
        file.mimetype === 'application/x-zip-compressed' ||
        file.originalname.endsWith('.zip')) {
      cb(null, true);
    } else {
      cb(new Error('Ð¢Ð¾Ð»ÑŒÐºÐ¾ ZIP Ñ„Ð°Ð¹Ð»Ñ‹ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ñ‹'));
    }
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// HELPER FUNCTIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function ensureBackupDir() {
  await fs.mkdir(BACKUP_DIR, { recursive: true });
}

// ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†, enum, Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹, view)
async function cleanDatabase() {
  return new Promise(async (resolve, reject) => {
    try {
      console.log('ðŸ§¹ Dropping all database objects...');
      
      await sequelize.query(`
        DO $$ 
        DECLARE
          r RECORD;
        BEGIN
          -- Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²ÑÐµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
          FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
            EXECUTE 'DROP TABLE IF EXISTS public.' || quote_ident(r.tablename) || ' CASCADE';
          END LOOP;
          
          -- Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²ÑÐµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
          FOR r IN (SELECT sequence_name FROM information_schema.sequences WHERE sequence_schema = 'public') LOOP
            EXECUTE 'DROP SEQUENCE IF EXISTS public.' || quote_ident(r.sequence_name) || ' CASCADE';
          END LOOP;
          
          -- Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²ÑÐµ ENUM Ñ‚Ð¸Ð¿Ñ‹
          FOR r IN (SELECT typname FROM pg_type WHERE typtype = 'e' AND typnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'public')) LOOP
            EXECUTE 'DROP TYPE IF EXISTS public.' || quote_ident(r.typname) || ' CASCADE';
          END LOOP;
          
          -- Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²ÑÐµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
          FOR r IN (SELECT proname, oidvectortypes(proargtypes) as argtypes 
                    FROM pg_proc INNER JOIN pg_namespace ON pg_proc.pronamespace = pg_namespace.oid 
                    WHERE pg_namespace.nspname = 'public') LOOP
            EXECUTE 'DROP FUNCTION IF EXISTS public.' || quote_ident(r.proname) || '(' || r.argtypes || ') CASCADE';
          END LOOP;
          
          -- Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²ÑÐµ view
          FOR r IN (SELECT viewname FROM pg_views WHERE schemaname = 'public') LOOP
            EXECUTE 'DROP VIEW IF EXISTS public.' || quote_ident(r.viewname) || ' CASCADE';
          END LOOP;
        END $$;
      `);
      
      console.log('âœ… Database cleaned successfully');
      resolve();
    } catch (error) {
      reject(new Error(`Database cleanup failed: ${error.message}`));
    }
  });
}

// Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ñ‹ Ð¸Ð· SQL Ñ„Ð°Ð¹Ð»Ð°
function runPsqlRestore(sqlFile) {
  return new Promise((resolve, reject) => {
    const args = [
      '-h', process.env.DB_HOST || 'localhost',
      '-p', process.env.DB_PORT || '5432',
      '-U', process.env.DB_USER,
      '-d', process.env.DB_NAME,
      '-f', sqlFile,
      '-v', 'ON_ERROR_STOP=1'
    ];

    const env = { ...process.env, PGPASSWORD: process.env.DB_PASSWORD };
    const psql = spawn('psql', args, { env, shell: true });
    
    let stderr = '';
    let stdout = '';
    
    psql.stdout.on('data', (data) => { stdout += data.toString(); });
    psql.stderr.on('data', (data) => { stderr += data.toString(); });

    psql.on('close', (code) => {
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`psql failed (code ${code}): ${stderr}`));
      }
    });

    psql.on('error', (err) => {
      reject(new Error(`psql error: ${err.message}`));
    });
  });
}

// Ð Ð°ÑÐ¿Ð°ÐºÐ¾Ð²ÐºÐ° ZIP Ð°Ñ€Ñ…Ð¸Ð²Ð°
async function extractZip(zipPath, extractDir) {
  return new Promise((resolve, reject) => {
    fsSync.createReadStream(zipPath)
      .pipe(unzipper.Extract({ path: extractDir }))
      .on('close', resolve)
      .on('error', reject);
  });
}

// ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
async function clearDirectory(dirPath) {
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      
      if (entry.isDirectory()) {
        await clearDirectory(fullPath);
        await fs.rmdir(fullPath);
      } else {
        await fs.unlink(fullPath);
      }
    }
  } catch (error) {
    if (error.code !== 'ENOENT') {
      throw error;
    }
  }
}

// ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
async function copyDir(src, dest) {
  await fs.mkdir(dest, { recursive: true });
  const entries = await fs.readdir(src, { withFileTypes: true });
  
  for (const entry of entries) {
    const srcPath = path.join(src, entry.name);
    const destPath = path.join(dest, entry.name);
    
    if (entry.isDirectory()) {
      await copyDir(srcPath, destPath);
    } else {
      await fs.copyFile(srcPath, destPath);
    }
  }
}

// ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð±ÑÐºÐ°Ð¿Ð°
async function validateBackup(extractDir) {
  const errors = [];
  
  const sqlFile = path.join(extractDir, 'database.sql');
  try {
    await fs.access(sqlFile);
  } catch {
    errors.push('ÐžÑ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ„Ð°Ð¹Ð» database.sql');
  }
  
  return errors;
}

// Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸ Ð´Ð»Ñ Ð¾Ñ‚ÐºÐ°Ñ‚Ð°
async function createTempBackup() {
  const tempBackupDir = path.join(BACKUP_DIR, `temp-${Date.now()}`);
  await fs.mkdir(tempBackupDir, { recursive: true });
  
  // ÐšÐ¾Ð¿Ð¸Ñ€ÑƒÐµÐ¼ uploads
  const tempUploads = path.join(tempBackupDir, 'uploads');
  try {
    await copyDir(UPLOADS_DIR, tempUploads);
  } catch (error) {
    console.log('Uploads folder not found, skipping backup');
  }
  
  // Ð”Ð°Ð¼Ð¿ Ð±Ð°Ð·Ñ‹
  const tempDump = path.join(tempBackupDir, 'database.sql');
  await new Promise((resolve, reject) => {
    const args = [
      '-h', process.env.DB_HOST || 'localhost',
      '-p', process.env.DB_PORT || '5432',
      '-U', process.env.DB_USER,
      '-d', process.env.DB_NAME,
      '-f', tempDump,
      '-F', 'p',
      '--no-owner',
      '--no-privileges'
    ];

    const env = { ...process.env, PGPASSWORD: process.env.DB_PASSWORD };
    const pgDump = spawn('pg_dump', args, { env, shell: true });
    
    let stderr = '';
    pgDump.stderr.on('data', (data) => { stderr += data.toString(); });

    pgDump.on('close', (code) => {
      if (code === 0) resolve();
      else reject(new Error(`pg_dump failed (code ${code}): ${stderr}`));
    });

    pgDump.on('error', (err) => {
      reject(new Error(`pg_dump error: ${err.message}`));
    });
  });
  
  return tempBackupDir;
}

// ÐžÑ‚ÐºÐ°Ñ‚ Ðº Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸
async function rollbackFromTemp(tempBackupDir) {
  console.log('âš ï¸ Rolling back to previous state...');
  
  try {
    const tempDump = path.join(tempBackupDir, 'database.sql');
    await cleanDatabase();
    await runPsqlRestore(tempDump);
    console.log('âœ… Database rolled back');
  } catch (error) {
    console.error('âŒ Rollback database error:', error.message);
  }
  
  try {
    const tempUploads = path.join(tempBackupDir, 'uploads');
    await clearDirectory(UPLOADS_DIR);
    await copyDir(tempUploads, UPLOADS_DIR);
    console.log('âœ… Files rolled back');
  } catch (error) {
    console.error('âŒ Rollback files error:', error.message);
  }
  
  await fs.rm(tempBackupDir, { recursive: true, force: true }).catch(() => {});
  console.log('âœ… Rollback completed');
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ROUTES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// List backups
router.get('/', authenticate, requireAdmin, async (req, res) => {
  try {
    await ensureBackupDir();
    const files = await fs.readdir(BACKUP_DIR);
    
    const backups = await Promise.all(
      files.filter(f => f.endsWith('.zip')).map(async filename => {
        const filepath = path.join(BACKUP_DIR, filename);
        const stat = await fs.stat(filepath);
        return {
          filename,
          size: stat.size,
          createdAt: stat.birthtime,
          isUploaded: filename.startsWith('uploaded-')
        };
      })
    );

    backups.sort((a, b) => new Date(b.createdAt) - new Date(a.createdAt));
    res.json(backups);
  } catch (error) {
    console.error('List backups error:', error);
    res.status(500).json({ error: 'Failed to list backups' });
  }
});

// Create backup
router.post('/', authenticate, requireAdmin, async (req, res) => {
  try {
    await ensureBackupDir();
    
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const filename = `backup-${timestamp}.zip`;
    const filepath = path.join(BACKUP_DIR, filename);
    const dumpFile = path.join(BACKUP_DIR, `db-${timestamp}.sql`);

    console.log('ðŸ“¦ Creating database dump...');
    
    await new Promise((resolve, reject) => {
      const args = [
        '-h', process.env.DB_HOST || 'localhost',
        '-p', process.env.DB_PORT || '5432',
        '-U', process.env.DB_USER,
        '-d', process.env.DB_NAME,
        '-f', dumpFile,
        '-F', 'p',
        '--no-owner',
        '--no-privileges'
      ];

      const env = { ...process.env, PGPASSWORD: process.env.DB_PASSWORD };
      const pgDump = spawn('pg_dump', args, { env, shell: true });
      
      let stderr = '';
      pgDump.stderr.on('data', (data) => { stderr += data.toString(); });

      pgDump.on('close', (code) => {
        if (code === 0) resolve();
        else reject(new Error(`pg_dump failed (code ${code}): ${stderr}`));
      });

      pgDump.on('error', (err) => {
        reject(new Error(`pg_dump error: ${err.message}`));
      });
    });
    
    console.log('âœ… Database dump created');

    console.log('ðŸ“¦ Creating archive...');
    const output = fsSync.createWriteStream(filepath);
    const archive = archiver('zip', { zlib: { level: 9 } });

    await new Promise((resolve, reject) => {
      output.on('close', () => {
        console.log(`âœ… Archive created: ${archive.pointer()} bytes`);
        resolve();
      });
      output.on('error', reject);
      archive.on('error', reject);

      archive.pipe(output);
      archive.file(dumpFile, { name: 'database.sql' });
      
      fs.access(UPLOADS_DIR)
        .then(() => {
          console.log('ðŸ“ Adding uploads folder...');
          archive.directory(UPLOADS_DIR, 'uploads');
          archive.finalize();
        })
        .catch(() => {
          console.log('â„¹ï¸ No uploads folder found');
          archive.finalize();
        });
    });

    await fs.unlink(dumpFile).catch(() => {});
    
    const stat = await fs.stat(filepath);
    console.log('ðŸŽ‰ Backup completed:', filename);
    
    res.json({ 
      message: 'Backup created successfully', 
      filename, 
      size: stat.size 
    });
  } catch (error) {
    console.error('âŒ Backup error:', error);
    res.status(500).json({ error: 'Failed to create backup: ' + error.message });
  }
});

// Upload backup
router.post('/upload', authenticate, requireAdmin, upload.single('backup'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }

    const stat = await fs.stat(req.file.path);
    
    res.json({
      message: 'Backup uploaded successfully',
      filename: req.file.filename,
      size: stat.size
    });
  } catch (error) {
    console.error('Upload backup error:', error);
    res.status(500).json({ error: 'Failed to upload backup: ' + error.message });
  }
});

// Restore backup
router.post('/restore/:filename', authenticate, requireAdmin, async (req, res) => {
  const { filename } = req.params;
  const { restoreDb = true, restoreFiles = true } = req.body;
  
  if (!filename.endsWith('.zip') || filename.includes('..') || filename.includes('/')) {
    return res.status(400).json({ error: 'Invalid filename' });
  }

  const filepath = path.join(BACKUP_DIR, filename);
  const extractDir = path.join(BACKUP_DIR, `restore-${Date.now()}`);
  let tempBackupDir = null;
  
  try {
    await fs.access(filepath);
    
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('ðŸ”„ Starting FULL RESTORE from:', filename);
    console.log('Options:', { restoreDb, restoreFiles });
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

    // 1. Ð Ð°ÑÐ¿Ð°ÐºÐ¾Ð²ÐºÐ°
    await fs.mkdir(extractDir, { recursive: true });
    console.log('ðŸ“¦ Extracting archive...');
    await extractZip(filepath, extractDir);
    console.log('âœ… Archive extracted');

    // 2. Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
    console.log('ðŸ” Validating backup...');
    const validationErrors = await validateBackup(extractDir);
    if (validationErrors.length > 0) {
      throw new Error('Backup validation failed: ' + validationErrors.join(', '));
    }
    console.log('âœ… Backup is valid');

    // 3. Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ ÐºÐ¾Ð¿Ð¸Ñ Ð´Ð»Ñ Ð¾Ñ‚ÐºÐ°Ñ‚Ð°
    console.log('ðŸ’¾ Creating temporary backup for rollback...');
    tempBackupDir = await createTempBackup();
    console.log('âœ… Temporary backup created');

    const results = { database: null, files: null };

    // 4. Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð‘Ð”
    if (restoreDb) {
      const sqlFile = path.join(extractDir, 'database.sql');
      try {
        console.log('ðŸ—„ï¸  Restoring database (full clean and restore)...');
        
        // ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: Ð¿Ð¾Ð»Ð½Ð°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ°
        await cleanDatabase();
        
        // Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼
        const { stdout, stderr } = await runPsqlRestore(sqlFile);
        
        results.database = 'success';
        console.log('âœ… Database restored successfully');
        
        if (stderr && stderr.includes('ERROR')) {
          console.warn('âš ï¸ Database restore warnings:', stderr.substring(0, 500));
        }
      } catch (dbError) {
        console.error('âŒ Database restore error:', dbError.message);
        
        await rollbackFromTemp(tempBackupDir);
        tempBackupDir = null;
        
        throw new Error('Database restore failed: ' + dbError.message);
      }
    }

    // 5. Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð¾Ð²
    if (restoreFiles) {
      const uploadsBackup = path.join(extractDir, 'uploads');
      try {
        await fs.access(uploadsBackup);
        console.log('ðŸ“ Restoring files (full clean and restore)...');
        
        console.log('ðŸ§¹ Cleaning uploads folder...');
        await clearDirectory(UPLOADS_DIR);
        console.log('âœ… Uploads folder cleaned');
        
        await fs.mkdir(UPLOADS_DIR, { recursive: true });
        await copyDir(uploadsBackup, UPLOADS_DIR);
        
        results.files = 'success';
        console.log('âœ… Files restored successfully');
      } catch (filesError) {
        if (filesError.code === 'ENOENT') {
          console.log('â„¹ï¸ No uploads in backup, cleaning current uploads...');
          await clearDirectory(UPLOADS_DIR);
          results.files = 'cleaned (no uploads in backup)';
        } else {
          console.error('âŒ Files restore error:', filesError.message);
          results.files = 'error: ' + filesError.message;
        }
      }
    }

    // 6. ÐžÑ‡Ð¸ÑÑ‚ÐºÐ°
    console.log('ðŸ§¹ Cleaning up...');
    await fs.rm(extractDir, { recursive: true, force: true });
    
    if (tempBackupDir) {
      await fs.rm(tempBackupDir, { recursive: true, force: true });
    }
    
    console.log('âœ… Cleanup completed');
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('ðŸŽ‰ RESTORE COMPLETED SUCCESSFULLY');
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

    const success = results.database === 'success' || results.files === 'success';
    
    res.json({
      message: success ? 'Restore completed successfully' : 'Restore completed with issues',
      results
    });
  } catch (error) {
    console.error('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.error('âŒ RESTORE FAILED:', error.message);
    console.error('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    
    await fs.rm(extractDir, { recursive: true, force: true }).catch(() => {});
    
    res.status(500).json({ 
      error: 'Failed to restore backup: ' + error.message,
      hint: 'Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð±Ñ‹Ð»Ð° Ð¾Ñ‚ÐºÐ°Ñ‡ÐµÐ½Ð° Ðº Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¼Ñƒ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ'
    });
  }
});

// Download backup
router.get('/download/:filename', authenticate, requireAdmin, async (req, res) => {
  try {
    const { filename } = req.params;
    
    if (!filename.endsWith('.zip') || filename.includes('..') || filename.includes('/')) {
      return res.status(400).json({ error: 'Invalid filename' });
    }

    const filepath = path.join(BACKUP_DIR, filename);
    await fs.access(filepath);
    res.download(filepath);
  } catch (error) {
    res.status(404).json({ error: 'Backup not found' });
  }
});

// Delete backup
router.delete('/:filename', authenticate, requireAdmin, async (req, res) => {
  try {
    const { filename } = req.params;
    
    if (!filename.endsWith('.zip') || filename.includes('..') || filename.includes('/')) {
      return res.status(400).json({ error: 'Invalid filename' });
    }

    const filepath = path.join(BACKUP_DIR, filename);
    await fs.unlink(filepath);
    res.json({ message: 'Backup deleted' });
  } catch (error) {
    res.status(404).json({ error: 'Backup not found' });
  }
});

// Clean old backups
router.post('/cleanup', authenticate, requireAdmin, async (req, res) => {
  try {
    const retentionDays = parseInt(process.env.BACKUP_RETENTION_DAYS) || 30;
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - retentionDays);

    await ensureBackupDir();
    const files = await fs.readdir(BACKUP_DIR);
    let deleted = 0;

    for (const filename of files) {
      if (!filename.endsWith('.zip')) continue;
      
      const filepath = path.join(BACKUP_DIR, filename);
      const stat = await fs.stat(filepath);
      
      if (stat.birthtime < cutoffDate) {
        await fs.unlink(filepath);
        deleted++;
      }
    }

    res.json({ message: `Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ ÑÑ‚Ð°Ñ€Ñ‹Ñ… ÐºÐ¾Ð¿Ð¸Ð¹: ${deleted}` });
  } catch (error) {
    console.error('Cleanup error:', error);
    res.status(500).json({ error: 'Failed to clean backups' });
  }
});

module.exports = router;